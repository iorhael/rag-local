import os
from pathlib import Path
import tempfile
from typing import List
import time

# Load environment variables from .env (if present)
from dotenv import load_dotenv

# Vector store and embedding imports
import pinecone as pinecone_module
from pinecone.db_data import Index as PineconeIndex
from langchain_community.vectorstores.pinecone import Pinecone as PineconeVectorStore
from langchain_openai.embeddings import OpenAIEmbeddings

# Document processing imports
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_text_splitters import CharacterTextSplitter

# LLM and chain imports
from langchain_classic.chains import ConversationalRetrievalChain
from langchain_community.chat_models import ChatOpenAI

from pinecone import ServerlessSpec, Pinecone as PineconeClient

import streamlit as st

# Ensure .env is loaded for local development
load_dotenv()

# langchain_community's Pinecone VectorStore expects `pinecone.Index` to exist.
# In pinecone>=8, the class lives in `pinecone.db_data.Index`, so we alias it at runtime.
if not hasattr(pinecone_module, "Index"):
    pinecone_module.Index = PineconeIndex

# Set up directory structure
TMP_DIR = Path(__file__).resolve().parent.joinpath('data', 'tmp')
TMP_DIR.mkdir(parents=True, exist_ok=True)

# Streamlit page configuration
st.set_page_config(page_title="RAG System")
st.title("ðŸ“š Document AI Assistant")
st.write("Upload your documents to get AI-powered insights.")

def validate_api_keys() -> bool:
    """Validate required API keys are present"""
    if not st.session_state.openai_api_key:
        st.error("Please enter your OpenAI API key")
        return False
    if not st.session_state.pinecone_api_key:
        st.error("Please enter your Pinecone API key")
        return False
    if not st.session_state.index_name:
        st.error("Please enter your Pinecone index name")
        return False
    return True

def cleanup_temp_files():
    """Clean up temporary files"""
    try:
        for file in TMP_DIR.glob('*'):
            file.unlink()
    except Exception as e:
        st.error(f"Error cleaning up files: {str(e)}")

def load_documents():
    """Load documents from temp directory"""
    loader = DirectoryLoader(TMP_DIR.as_posix(), glob='**/*.pdf', loader_cls=PyPDFLoader)
    try:
        documents = loader.load()
        return documents
    except Exception as e:
        st.error(f"Error loading documents: {str(e)}")
        return []

def split_documents(documents, chunk_size=1000, chunk_overlap=200):
    """Split documents into chunks with overlap"""
    text_splitter = CharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separator="\n"
    )
    return text_splitter.split_documents(documents)

def create_vectorstore(texts):
    """Create Pinecone vectorstore"""
    try:
        embeddings = OpenAIEmbeddings(
            openai_api_key=st.session_state.openai_api_key,
            model="text-embedding-3-small",
        )

        pc = PineconeClient(api_key=st.session_state.pinecone_api_key)

        # Create index if it doesn't exist
        if st.session_state.index_name not in pc.list_indexes().names():
            pc.create_index(
                name=st.session_state.index_name,
                dimension=1536,
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1")
            )
            # Wait for index to be ready
            time.sleep(20)

        index = pc.Index(st.session_state.index_name)
        vector_store = PineconeVectorStore(index, embeddings, text_key="text")

        text_contents = [doc.page_content for doc in texts]
        metadatas = [dict(doc.metadata) for doc in texts]
        vector_store.add_texts(text_contents, metadatas=metadatas)

        return vector_store.as_retriever()

    except Exception as e:
        st.error(f"Error creating vectorstore: {str(e)}")
        return None

def query_llm(retriever, query):
    """Process queries using the retrieval chain"""
    try:
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=ChatOpenAI(
                openai_api_key=st.session_state.openai_api_key,
                temperature=0.7,
                model="gpt-4-turbo-preview"
            ),
            retriever=retriever,
            return_source_documents=True
        )

        result = qa_chain({
            'question': query,
            'chat_history': st.session_state.messages
        })

        st.session_state.messages.append((query, result['answer']))
        return result['answer']
    except Exception as e:
        st.error(f"Error processing query: {str(e)}")
        return "Error processing your question. Please try again."

def main():
    # Initialize session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "retriever" not in st.session_state:
        st.session_state.retriever = None

    # Sidebar configuration
    with st.sidebar:
        st.session_state.openai_api_key = (
            st.secrets.get("OPENAI_API_KEY")
            or os.getenv("OPENAI_API_KEY")
            or st.text_input("OpenAI API Key", type="password")
        )
        st.session_state.pinecone_api_key = (
            st.secrets.get("PINECONE_API_KEY")
            or os.getenv("PINECONE_API_KEY")
            or st.text_input("Pinecone API Key", type="password")
        )
        st.session_state.index_name = (
            st.secrets.get("PINECONE_INDEX")
            or os.getenv("PINECONE_INDEX")
            or st.text_input("Pinecone Index Name")
        )

        if st.button("Clear Chat"):
            st.session_state.messages = []
            st.rerun()

    # Validate keys before processing
    if not validate_api_keys():
        return

    # File upload
    st.session_state.uploaded_files = st.file_uploader(
        "Upload Documents",
        type=["pdf"],
        accept_multiple_files=True
    )

    # Process documents button
    if st.button("Process Documents"):
        if not st.session_state.uploaded_files:
            st.warning("Please upload documents first.")
            return

        with st.spinner("Processing documents..."):
            # Save uploaded files
            for uploaded_file in st.session_state.uploaded_files:
                with tempfile.NamedTemporaryFile(
                    delete=False,
                    dir=TMP_DIR.as_posix(),
                    suffix='.pdf'
                ) as tmp_file:
                    tmp_file.write(uploaded_file.read())

            # Process documents
            documents = load_documents()
            texts = split_documents(documents)
            st.session_state.retriever = create_vectorstore(texts)

            # Cleanup temp files
            cleanup_temp_files()

        st.success("Documents processed successfully!")

    # Display chat interface
    for message in st.session_state.messages:
        st.chat_message('human').write(message[0])
        st.chat_message('assistant').write(message[1])

    # Chat input
    if query := st.chat_input():
        if "retriever" not in st.session_state or st.session_state.retriever is None:
            st.warning("Please process documents first.")
            return

        st.chat_message("human").write(query)
        response = query_llm(st.session_state.retriever, query)
        st.chat_message("assistant").write(response)

if __name__ == '__main__':
    main()
